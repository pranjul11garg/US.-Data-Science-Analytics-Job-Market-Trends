[
  {
    "objectID": "analytics_model.html",
    "href": "analytics_model.html",
    "title": "analytcis model",
    "section": "",
    "text": "Code\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import adjusted_rand_score\neda = pd.read_csv(\"data/eda_data.csv\")\n\n\n\n\nCode\n\neda = pd.read_csv(\"data/eda_data.csv\")\n\nfeatures = eda[['SALARY', 'MAX_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE']].copy()\n\nfor col in ['MAX_YEARS_EXPERIENCE', 'MIN_YEARS_EXPERIENCE', 'SALARY']:\n    features[col] = pd.to_numeric(features[col], errors='coerce')\n\nfeatures = features.dropna()\n\nscaler = StandardScaler()\nX = scaler.fit_transform(features)\n\nkmeans = KMeans(n_clusters=4, random_state=688)\neda.loc[features.index, 'Cluster'] = kmeans.fit_predict(X)\n\ntrue_labels = eda.loc[features.index, 'SOC_2021_4_NAME']\ntrue_labels_encoded = LabelEncoder().fit_transform(true_labels)\n\nari = adjusted_rand_score(true_labels_encoded, eda.loc[features.index, 'Cluster'])\n\n\n\n\nCode\nimport plotly.express as px\nplt.figure(figsize=(10, 6))\n\n# Scatter plot of salary vs. max experience, colored by cluster\nsns.scatterplot(\n    x=features['SALARY'],\n    y=features['MAX_YEARS_EXPERIENCE'],\n    hue=eda.loc[features.index, 'Cluster'],\n    palette='Set2',\n    s=40,\n    edgecolor='white',\n    linewidth=0.5\n)\n\n# Plot centroids\ncentroids = kmeans.cluster_centers_\nplt.scatter(\n    centroids[:, 0] * X.std(axis=0)[0] + X.mean(axis=0)[0],\n    centroids[:, 1] * X.std(axis=0)[1] + X.mean(axis=0)[1],\n    marker='X',\n    s=200,\n    c='black',\n    label='Centroids'\n)\n\n# Titles and labels\nplt.title(\"KMeans Clustering by Salary and Max Years Experience\", fontsize=16)\nplt.xlabel(\"Salary\", fontsize=12)\nplt.ylabel(\"Max Years Experience\", fontsize=12)\nplt.legend(title='Cluster', loc='upper right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom IPython.display import HTML\n\n# 1) Build the DataFrame\ndf_plot = features.copy()\ndf_plot['Cluster'] = eda.loc[features.index, 'Cluster']\n\n# 2) Compute centroids in original units\ncentroids = kmeans.cluster_centers_\ncentroids_x = centroids[:, 0] * X.std(axis=0)[0] + X.mean(axis=0)[0]\ncentroids_y = centroids[:, 1] * X.std(axis=0)[1] + X.mean(axis=0)[1]\n\n# 3) Create an interactive Plotly Figure\nfig = px.scatter(\n    df_plot,\n    x='SALARY',\n    y='MAX_YEARS_EXPERIENCE',\n    color='Cluster',\n    title=\"KMeans Clustering by Salary and Max Years Experience\",\n    labels={\n        'SALARY': 'Salary',\n        'MAX_YEARS_EXPERIENCE': 'Max Years Experience',\n        'Cluster': 'Cluster'\n    },\n    width=800,\n    height=500,\n)\n\n# 4) Add centroid traces\nfig.add_trace(\n    go.Scatter(\n        x=centroids_x,\n        y=centroids_y,\n        mode='markers',\n        marker=dict(symbol='x', size=18, color='black', line=dict(width=2, color='white')),\n        name='Centroids'\n    )\n)\n\n# 5) Render the full HTML and embed it\nhtml = fig.to_html(include_plotlyjs='cdn')\nHTML(html)\n\n\n\n\n\n                            \n                                            \n\n\n\n\nHere we have 4 cluster groups. Group 0, which represent as green have lower salary, mostly under 150k, and max years experience in 2-5 years, it is likely Likely junior to mid-level employees with moderate pay. Group 1 with orange, has medium to high salary, wide range from $100k–$500k and with narrow range ~3 years, they are suggests specialized or high-paying roles with short experience — possibly fast-track promotions or high-demand fields. cluster 2 are low salary and experience from 0-4 years, they are clearly entry level employee. cluster 3 has medium salary, mostly under 200k with higher experiences, like 6-13 eyars. They probably are senior professionals with more experience but not the highest salaries.\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfeatures1 = eda[['MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE']].copy()\n\nfor col in features.columns:\n    features[col] = pd.to_numeric(features[col], errors='coerce')\n\nfeatures = features.dropna()\n\nX = features\ny = eda.loc[X.index, 'SALARY']\n\nX = X.dropna()\ny = y.loc[X.index]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=688)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n\n\n\n\nCode\nplt.figure(figsize =(10,6))\nplt.scatter(y_test, y_pred, alpha = 0.6, color = 'skyblue')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', lw=2)\nplt.xlabel(\"Actual Salary\")\nplt.ylabel(\"Predicted Salary\")\nplt.title(\"Actual vs Predicted Salary (Multiple Regression)\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThis plot shows the Actual vs. Predicted Salary using a multiple linear regression model. The blue dots represent individual predictions, and the red dashed line is the ideal line where predicted = actual. Since most points lie very close to the red line, it means your model predicts salary very accurately, with minimal error and strong linear fit — likely reflected in a high R² score near 1.0."
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This page presents our data cleaning and prepping part.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport plotly.express as px\nimport numpy as np\nimport plotly.graph_objects as go\n\ndf = pd.read_csv(\"./data/lightcast_job_postings.csv\")\n\ncolumns_to_keep = [\n    'COMPANY', 'LOCATION', 'POSTED', 'MIN_EDULEVELS_NAME', 'MAX_EDULEVELS_NAME',\n    'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'TITLE', 'SKILLS',\n    'SPECIALIZED_SKILLS', 'CERTIFICATIONS', 'COMMON_SKILLS', 'SOFTWARE_SKILLS',\n    'SOC_2021_4_NAME', 'NAICS_2022_6', 'NAICS2_NAME', 'REMOTE_TYPE_NAME',\n    'SALARY', 'TITLE_NAME', 'SKILLS_NAME', 'SPECIALIZED_SKILLS_NAME'\n]\neda_data = df[columns_to_keep]\n\n\n\n\nCode\nmissing_matrix = eda_data.isnull().astype(int)\ncorr = missing_matrix.corr().round(2)\n\nmask = np.triu(np.ones(corr.shape), k=1).astype(bool)\nmasked_corr = corr.mask(mask)\n\ntext_labels = masked_corr.astype(str)\ntext_labels[masked_corr.isna()] = \"\"\n\n# plot\nfig = go.Figure(data=go.Heatmap(\n    z=masked_corr.values,\n    x=masked_corr.columns,\n    y=masked_corr.index,\n    text=text_labels.values,\n    texttemplate=\"%{text}\",\n    colorscale=\"Blues\",\n    colorbar=dict(title=\"Missing Corr\"),\n    zmin=0,\n    zmax=1,\n    hoverinfo='skip'\n))\n\nfig.update_layout(\n    title=\"Clean Triangle Missing Value Correlation Heatmap\",\n    xaxis_tickangle=45,\n    width=850,\n    height=600,\n    margin=dict(t=50, l=80, r=50, b=80),\n    font=dict(size=8),\n    plot_bgcolor='white'\n)\n\nfig.update_yaxes(autorange=\"reversed\")\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nThis triangle heatmap visualizes the correlation of missing values between different columns in the dataset. Each square represents how often two columns are missing together, with darker blue indicating a stronger relationship. Most of the values are very high (close to 1.0), suggesting that when one column is missing, others are often missing too — especially among skill-related fields like SKILLS, SPECIALIZED_SKILLS, and SOFTWARE_SKILLS, which are likely part of the same job posting metadata.\nThis pattern indicates that missingness is not random, but structured — possibly due to differences in how job descriptions are recorded across roles or industries. For example, a job with no software skill tags might also lack common skills or NAICS codes, hinting at data input gaps rather than actual job content differences. Recognizing these correlations is helpful for choosing imputation strategies or deciding whether to drop certain rows or columns entirely during preprocessing.\n\n\nCode\nif \"SALARY\" in eda_data.columns:\n    eda_data[\"SALARY\"].fillna(eda_data[\"SALARY\"].median(), inplace=True)\nelse:\n    print(\"Warning: 'SALARY' column not found in dataframe!\")\n\nif \"COMPANY\" in eda_data.columns:\n    eda_data[\"COMPANY\"].fillna(\"Unknown\", inplace=True)\nelse:\n    print(\"Warning: 'COMPANY' column not found in dataframe!\")\n\n    # Fill numeric columns with mean\nnum_cols = eda_data.select_dtypes(include='number').columns\nfor col in num_cols:\n    if eda_data[col].isnull().sum() &gt; 0:\n        eda_data[col].fillna(eda_data[col].mean(), inplace=True)\n\n# Fill categorical columns with mode\ncat_cols = eda_data.select_dtypes(include='object').columns\nfor col in cat_cols:\n    if eda_data[col].isnull().sum() &gt; 0:\n        eda_data[col].fillna(eda_data[col].mode()[0], inplace=True)\n\neda_data.dropna(thresh=len(eda_data) * 0.5, axis=1, inplace=True)\n\n\n# delete duplicates\neda_data = eda_data.drop_duplicates(subset=[\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"])"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Trends in Data Science & Business Analytics!",
    "section": "",
    "text": "Welcome to our deep dive into the evolving landscape of Data Science and Business Analytics in the United States. As industries across the nation increasingly rely on data-driven insights, the role of data science and analytics has become paramount in shaping business strategies, optimizing operations, and driving innovation.\nThe rise of artificial intelligence (AI) is reshaping the landscape of data science and business analytics in the United States, creating both opportunities and challenges for workers and businesses alike. As AI continues to evolve, it has sparked widespread concern over job displacement. According to a recent report from the World Economic Forum, 86% of workers express concerns about AI potentially leading to job losses, with predictions of significant industry shifts occurring within the next two to five years (Samuels (2024)). However, business leaders emphasize that AI is not merely a tool for job replacement—it’s a transformative force that is changing the nature of work itself. For instance, Rakuten’s partnership with OpenAI to create an internal version of ChatGPT is helping employees work more efficiently, focusing on higher-level tasks rather than being displaced by technology (Samuels (2024)).\n\n\n\nData Science Image\n\n\nOne key trend in 2024 is the growing demand for AI-related skills across various industries. A recent Gartner report highlights that many organizations are significantly increasing their investments in AI, with companies projected to spend an average of $2.5 million on AI integration in 2024 (Gartner (2024)). This investment is reshaping job descriptions and workforce requirements, as roles now demand proficiency in AI tools. For example, in software development, GitHub Copilot has been shown to increase programmers’ efficiency by 55%, enabling them to focus on complex problem-solving rather than repetitive coding tasks (GitHub (2023)). As a result, professionals who upskill and learn to work alongside AI will have better career prospects in the future.\n\n\n\nData Science Image\n\n\nThe career outlook for business analytics and data science professionals remains optimistic, as companies increasingly look to combine AI with human decision-making to enhance productivity and innovation. Industry leaders, including Bev White, CEO of Nash Squared, emphasize that AI is not about eliminating jobs, but rather reshaping them to make work more productive and meaningful (White (2024)). Similarly, PepsiCo’s CIO, Nigel Richardson, believes that while some jobs will be replaced, AI will ultimately create more opportunities than it eliminates (Richardson (2024)). This aligns with a McKinsey report that forecasts AI could generate up to $13 trillion in additional global GDP by 2030, creating new job opportunities in data science, business analytics, and other technology-related fields (Company (2023)).\nThis website explores the latest trends, key developments, and future directions in Data Science and Business Analytics in the U.S. We delve into emerging technologies, the growing demand for skilled professionals, and the impact these fields are having on various industries. Our aim is to provide you with valuable insights into how data science is shaping the future of business and the economy.\nJoin us as we explore:\n\nThe role of machine learning, AI, and automation in business analytics.\nHow data-driven decisions are driving competitive advantages across industries.\nThe growing importance of data ethics and privacy concerns.\nKey skills and tools shaping the future of data professionals.\n\n\n\n\n\nReferences\n\nCompany, M. &. (2023): “The Economic Potential of Generative AI: The Next Productivity Frontier,”https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier.\n\n\nGartner. (2024): “Marketing Budgets: Benchmarks for CMOs in the Era of Less,”https://www.gartner.com/en/marketing/topics/marketing-budget.\n\n\nGitHub. (2023): “GitHub Copilot: Your AI Pair Programmer,”https://github.com/features/copilot.\n\n\nRichardson, N. (2024): “CIO Interview: Nigel Richardson, European CIO, PepsiCo,”https://www.computerweekly.com/news/366570412/CIO-interview-Nigel-Richardson-European-CIO-PepsiCo.\n\n\nSamuels, M. (2024): “AI’s Employment Impact: 86% of Workers Fear Job Losses, but Here’s Some Good News,”https://www.zdnet.com/article/ai-employment-impact-86-of-workers-fear-job-losses-but-heres-some-good-news/.\n\n\nWhite, B. (2024): “The Future of Work: How AI Is Reshaping Careers,”https://www.harveynash.co.uk/team/bev-white."
  },
  {
    "objectID": "rm_model.html",
    "href": "rm_model.html",
    "title": "Random Forest Classification for ML/Data Science Requirement",
    "section": "",
    "text": "import pandas as pd\n\ndf = pd.read_csv(\"data/lightcast_job_postings.csv\")\ndf.columns\n\nIndex(['ID', 'LAST_UPDATED_DATE', 'LAST_UPDATED_TIMESTAMP', 'DUPLICATES',\n       'POSTED', 'EXPIRED', 'DURATION', 'SOURCE_TYPES', 'SOURCES', 'URL',\n       ...\n       'NAICS_2022_2', 'NAICS_2022_2_NAME', 'NAICS_2022_3',\n       'NAICS_2022_3_NAME', 'NAICS_2022_4', 'NAICS_2022_4_NAME',\n       'NAICS_2022_5', 'NAICS_2022_5_NAME', 'NAICS_2022_6',\n       'NAICS_2022_6_NAME'],\n      dtype='object', length=131)\n\n\n\nml_keywords = [\"machine learning\", \"data science\", \"ai\", \"artificial intelligence\", \"deep learning\", \"data scientist\"]\n\ndef requires_ml(skills):\n    if pd.isnull(skills):\n        return 0\n    skills = skills.lower()\n    return int(any(kw in skills for kw in ml_keywords))\n\ndf[\"REQUIRES_ML\"] = df[\"SKILLS_NAME\"].apply(requires_ml)\n\n\nfeatures = [\"TITLE\", \"SOC_2021_4_NAME\", \"NAICS2_NAME\", \"MIN_EDULEVELS_NAME\", \"MIN_YEARS_EXPERIENCE\"]\ntarget = \"REQUIRES_ML\"\n\ndf = df[features + [target, 'BODY']].dropna()\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\ndf_encoded = df.copy()\nlabel_encoders = {}\n\nfor col in features:\n    if df_encoded[col].dtype == \"object\":\n        le = LabelEncoder()\n        df_encoded[col] = le.fit_transform(df_encoded[col])\n        label_encoders[col] = le\n\n\nfrom sklearn.model_selection import train_test_split\n\nX = df_encoded[features]\ny = df_encoded[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.76      0.76      0.76      5257\n           1       0.72      0.72      0.72      4614\n\n    accuracy                           0.74      9871\n   macro avg       0.74      0.74      0.74      9871\nweighted avg       0.74      0.74      0.74      9871\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 5))\nplt.barh(features, rf.feature_importances_)\nplt.xlabel(\"Importance\")\nplt.title(\"Feature Importance - ML Role Classification\")\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Cleaned job descriptions\ndf['BODY_clean'] = df['BODY'].fillna(\"\").str.lower()\n\n# Target\ny = df['REQUIRES_ML']  # this should be a binary 1/0 column\n\n# TF-IDF vectorization\ntfidf = TfidfVectorizer(max_features=5000, stop_words='english')\nX = tfidf.fit_transform(df['BODY_clean'])\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.91      0.94      0.92      6644\n           1       0.92      0.89      0.90      5694\n\n    accuracy                           0.91     12338\n   macro avg       0.91      0.91      0.91     12338\nweighted avg       0.91      0.91      0.91     12338\n\n\n\n\nimport numpy as np\n\nimportances = model.feature_importances_\ntop_idx = np.argsort(importances)[-20:]\ntop_words = [tfidf.get_feature_names_out()[i] for i in top_idx]\n\nplt.barh(top_words, importances[top_idx])\nplt.title(\"Top Words for ML Role Classification\")\nplt.xlabel(\"Importance\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Generate confusion matrix\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n\n# Plot\ndisp.plot(cmap=\"Blues\")\nplt.title(\"Confusion Matrix - ML Role Classification\")\nplt.show()\n\n\n\n\n\n\n\n\nWe selected a combination of structured and unstructured features to predict whether a job role requires Machine Learning or Data Science. Structured features such as TITLE, SOC_2021_4_NAME, NAICS2_NAME, MIN_EDULEVELS_NAME, and MIN_YEARS_EXPERIENCE were chosen based on domain relevance—these fields reflect the role’s function, industry, required education, and experience level, all of which can signal ML-related requirements. Additionally, we included the job description BODY text, applying TF-IDF vectorization to extract key terms. This allowed the model to learn from nuanced language patterns within postings. Feature importance and performance metrics confirm that both structured metadata and text data contribute meaningfully to classification accuracy."
  }
]