[
  {
    "objectID": "data_analysis.html",
    "href": "data_analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "This document outlines the data cleaning process, including: - Handling missing values - Dropping unnecessary columns - Deduplicating records\n\n\nThe Lightcast Dataset has been extracted from the google drive using the gdown.\n\n\nCode\nimport pandas as pd\nimport gdown\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport gdown\nimport zipfile\nimport os\n\n\nfile_id = \"1VNBTxArDMN2o9fJBDImaON6YUAyJGOU6\" \nzip_file = \"lightcast_job_postings.zip\"  # Name of the downloaded ZIP file\ncsv_file = \"./data/lightcast_job_postings.csv\"  # Path to the CSV file\n\ngdown.download(f\"https://drive.google.com/uc?id={file_id}\", zip_file, quiet=True)\n\nwith zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n    zip_ref.extractall(\"./data\") \ndf = pd.read_csv(csv_file)\n\n\n\n\n\nThere are 102 columns in the dataset and there are various columns which are unnecessary, so we are dropping those to clean the data and do better analysis\n\n\nCode\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\ndf.drop(columns=columns_to_drop, inplace=True)\nprint(\"Dropped unnecessary columns.\")\n\n\nDropped unnecessary columns.\n\n\n\n\n\n\n\nCode\nimport missingno as msno\nimport matplotlib.pyplot as plt\n\n\ndf.columns = df.columns.str.upper().str.strip()  # Normalize column names\n\nmsno.heatmap(df)\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n\n# Drop columns with &gt;50% missing values\ndf.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n\ndf = df.drop_duplicates(subset=[\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"])\nprint(\"Duplicates removed.\")\nprint(\"✅ Missing value handling complete.\")\n\n\n\n\n\n\n\n\n\nDuplicates removed.\n✅ Missing value handling complete.\n\n\n\n\n\n\n\n\n\nCode\n# identifying data analyst jobs by keyword searching\nkeywords = ['Data Analyst', 'Business Analyst', 'Data Engineering', 'Deep Learning',\n            'Data Science', 'Data Analysis','Data Analytics',  'Market Research Analyst' \n            'LLM', 'Language Model', 'NLP', 'Natural Language Processing',\n            'Computer Vision', 'Business Intelligence Analyst', 'Quantitative Analyst', 'Operations Analyst']\n\nmatch = lambda col: df[col].str.contains('|'.join(keywords), case=False, na=False)\n\ndf['DATA_ANALYST_JOB'] = match('TITLE_NAME') \\\n             | match('SKILLS_NAME') \\\n             | match('SPECIALIZED_SKILLS_NAME') \ndf['DATA_ANALYST_JOB'].value_counts()\n\n\nDATA_ANALYST_JOB\nFalse    37052\nTrue     32148\nName: count, dtype: int64\n\n\n\n\nCode\nimport plotly.express as px\n\ndf_grouped = df.groupby(['DATA_ANALYST_JOB', 'NAICS2_NAME']).size().reset_index(name='Job_Count')\n# Define custom colors for 'True' and 'False' categories\ncolor_map = {\n    False: \"#d62728\" ,   # Red\n    True: \"#3BB143 \"   # Green\n}\n\n\nfig = px.bar(df_grouped, \n             x='NAICS2_NAME', \n             y='Job_Count',\n             color='DATA_ANALYST_JOB',\n             title=\"Data Analytics & Business Analytics Job Trends\",\n              labels={'NAICS2_NAME': 'Industry', 'Job_Count': 'Number of Jobs'},\n             barmode='group',\n             color_discrete_map=color_map  # Custom colors\n            )\n\nfig.update_layout(\n    yaxis=dict(\n        title=\"Number of Jobs\", \n        range=[0, df_grouped['Job_Count'].max() * 1.2]  # Increase y-axis height\n    ),\n    height=700,  # Make the figure taller\n    xaxis=dict(\n        tickangle=-45  # Rotate x-axis labels for better readability\n    )\n)\nfig.write_html(\"figures/plot1_jobtrends.html\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport plotly.express as px\n\n# Identify the top 2 industries with the most data analyst jobs\ntop_industries = df_grouped.groupby(\"NAICS2_NAME\")[\"Job_Count\"].sum().nlargest(10).index\n\n# Filter the original grouped DataFrame, not the raw df\ndf_top_industries = df_grouped[df_grouped[\"NAICS2_NAME\"].isin(top_industries)]\n\n# Create the bar chart to analyze job counts within these industries\nfig = px.bar(df_top_industries, \n             x=\"DATA_ANALYST_JOB\", \n             y=\"Job_Count\", \n             color=\"NAICS2_NAME\",\n             title=\"Top 10 Industries Hiring Data Analysts\",\n             labels={'DATA_ANALYST_JOB': 'Job Title', 'Job_Count': 'Number of Jobs'},\n             barmode='group')\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport plotly.express as px\n\n# Define target industries\ntarget_industries = [\"Finance and Insurance\", \"Health Care and Social Assistance\"]\n\n# Filter the original DataFrame (df) instead of df_grouped\ndf_remote = df[df[\"NAICS2_NAME\"].isin(target_industries)]\n\n# Count occurrences of each remote type in these industries\ndf_remote_grouped = df_remote.groupby(\"REMOTE_TYPE_NAME\").size().reset_index(name=\"Count\")\n\n# Create a pie chart\nfig = px.pie(df_remote_grouped, \n             names=\"REMOTE_TYPE_NAME\", \n             values=\"Count\", \n             title=\"Remote vs. On-Site Data Analyst Jobs in Finance & Healthcare\",\n             color=\"REMOTE_TYPE_NAME\", \n             color_discrete_map={\"Remote\": \"#1f77b4\", \"On-Site\": \"#d62728\", \"Hybrid\": \"#2ca02c\"})"
  },
  {
    "objectID": "data_analysis.html#extracting-the-dataset",
    "href": "data_analysis.html#extracting-the-dataset",
    "title": "Data Analysis",
    "section": "",
    "text": "The Lightcast Dataset has been extracted from the google drive using the gdown.\n\n\nCode\nimport pandas as pd\nimport gdown\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport gdown\nimport zipfile\nimport os\n\n\nfile_id = \"1VNBTxArDMN2o9fJBDImaON6YUAyJGOU6\" \nzip_file = \"lightcast_job_postings.zip\"  # Name of the downloaded ZIP file\ncsv_file = \"./data/lightcast_job_postings.csv\"  # Path to the CSV file\n\ngdown.download(f\"https://drive.google.com/uc?id={file_id}\", zip_file, quiet=True)\n\nwith zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n    zip_ref.extractall(\"./data\") \ndf = pd.read_csv(csv_file)"
  },
  {
    "objectID": "data_analysis.html#dropping-the-unnecessary-columns-columns",
    "href": "data_analysis.html#dropping-the-unnecessary-columns-columns",
    "title": "Data Analysis",
    "section": "",
    "text": "There are 102 columns in the dataset and there are various columns which are unnecessary, so we are dropping those to clean the data and do better analysis\n\n\nCode\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\ndf.drop(columns=columns_to_drop, inplace=True)\nprint(\"Dropped unnecessary columns.\")\n\n\nDropped unnecessary columns."
  },
  {
    "objectID": "data_analysis.html#removed-the-duplicates-and-handled-the-missing-values",
    "href": "data_analysis.html#removed-the-duplicates-and-handled-the-missing-values",
    "title": "Data Analysis",
    "section": "",
    "text": "Code\nimport missingno as msno\nimport matplotlib.pyplot as plt\n\n\ndf.columns = df.columns.str.upper().str.strip()  # Normalize column names\n\nmsno.heatmap(df)\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n\n# Drop columns with &gt;50% missing values\ndf.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n\ndf = df.drop_duplicates(subset=[\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"])\nprint(\"Duplicates removed.\")\nprint(\"✅ Missing value handling complete.\")\n\n\n\n\n\n\n\n\n\nDuplicates removed.\n✅ Missing value handling complete."
  },
  {
    "objectID": "data_analysis.html#exploratory-data-analysis",
    "href": "data_analysis.html#exploratory-data-analysis",
    "title": "Data Analysis",
    "section": "",
    "text": "Code\n# identifying data analyst jobs by keyword searching\nkeywords = ['Data Analyst', 'Business Analyst', 'Data Engineering', 'Deep Learning',\n            'Data Science', 'Data Analysis','Data Analytics',  'Market Research Analyst' \n            'LLM', 'Language Model', 'NLP', 'Natural Language Processing',\n            'Computer Vision', 'Business Intelligence Analyst', 'Quantitative Analyst', 'Operations Analyst']\n\nmatch = lambda col: df[col].str.contains('|'.join(keywords), case=False, na=False)\n\ndf['DATA_ANALYST_JOB'] = match('TITLE_NAME') \\\n             | match('SKILLS_NAME') \\\n             | match('SPECIALIZED_SKILLS_NAME') \ndf['DATA_ANALYST_JOB'].value_counts()\n\n\nDATA_ANALYST_JOB\nFalse    37052\nTrue     32148\nName: count, dtype: int64\n\n\n\n\nCode\nimport plotly.express as px\n\ndf_grouped = df.groupby(['DATA_ANALYST_JOB', 'NAICS2_NAME']).size().reset_index(name='Job_Count')\n# Define custom colors for 'True' and 'False' categories\ncolor_map = {\n    False: \"#d62728\" ,   # Red\n    True: \"#3BB143 \"   # Green\n}\n\n\nfig = px.bar(df_grouped, \n             x='NAICS2_NAME', \n             y='Job_Count',\n             color='DATA_ANALYST_JOB',\n             title=\"Data Analytics & Business Analytics Job Trends\",\n              labels={'NAICS2_NAME': 'Industry', 'Job_Count': 'Number of Jobs'},\n             barmode='group',\n             color_discrete_map=color_map  # Custom colors\n            )\n\nfig.update_layout(\n    yaxis=dict(\n        title=\"Number of Jobs\", \n        range=[0, df_grouped['Job_Count'].max() * 1.2]  # Increase y-axis height\n    ),\n    height=700,  # Make the figure taller\n    xaxis=dict(\n        tickangle=-45  # Rotate x-axis labels for better readability\n    )\n)\nfig.write_html(\"figures/plot1_jobtrends.html\")"
  },
  {
    "objectID": "data_analysis.html#data-analytics-business-analytics-job-trends",
    "href": "data_analysis.html#data-analytics-business-analytics-job-trends",
    "title": "Data Analysis",
    "section": "",
    "text": "Code\nimport plotly.express as px\n\n# Identify the top 2 industries with the most data analyst jobs\ntop_industries = df_grouped.groupby(\"NAICS2_NAME\")[\"Job_Count\"].sum().nlargest(10).index\n\n# Filter the original grouped DataFrame, not the raw df\ndf_top_industries = df_grouped[df_grouped[\"NAICS2_NAME\"].isin(top_industries)]\n\n# Create the bar chart to analyze job counts within these industries\nfig = px.bar(df_top_industries, \n             x=\"DATA_ANALYST_JOB\", \n             y=\"Job_Count\", \n             color=\"NAICS2_NAME\",\n             title=\"Top 10 Industries Hiring Data Analysts\",\n             labels={'DATA_ANALYST_JOB': 'Job Title', 'Job_Count': 'Number of Jobs'},\n             barmode='group')"
  },
  {
    "objectID": "data_analysis.html#data-analytics-business-analytics-job-trends-1",
    "href": "data_analysis.html#data-analytics-business-analytics-job-trends-1",
    "title": "Data Analysis",
    "section": "",
    "text": "Code\nimport plotly.express as px\n\n# Define target industries\ntarget_industries = [\"Finance and Insurance\", \"Health Care and Social Assistance\"]\n\n# Filter the original DataFrame (df) instead of df_grouped\ndf_remote = df[df[\"NAICS2_NAME\"].isin(target_industries)]\n\n# Count occurrences of each remote type in these industries\ndf_remote_grouped = df_remote.groupby(\"REMOTE_TYPE_NAME\").size().reset_index(name=\"Count\")\n\n# Create a pie chart\nfig = px.pie(df_remote_grouped, \n             names=\"REMOTE_TYPE_NAME\", \n             values=\"Count\", \n             title=\"Remote vs. On-Site Data Analyst Jobs in Finance & Healthcare\",\n             color=\"REMOTE_TYPE_NAME\", \n             color_discrete_map={\"Remote\": \"#1f77b4\", \"On-Site\": \"#d62728\", \"Hybrid\": \"#2ca02c\"})"
  },
  {
    "objectID": "backup/analysis.html",
    "href": "backup/analysis.html",
    "title": "Group 11: Trends in Data Science & Business Analytics",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\ndf = pd.read_csv(\"lightcast_job_postings.csv\")\nprint(\"Available columns in dataset:\", df.columns.tolist())\n\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\ndf.drop(columns=columns_to_drop, inplace=True)\nprint(\"Dropped unnecessary columns.\")\nprint(df.columns)\n\n# handle missing value\nprint(\"Missing values before cleaning:\")\nprint(df.isnull().sum())\n\n\n\nimport missingno as msno\nimport matplotlib.pyplot as plt\n\n# Check column names\ndf.columns = df.columns.str.upper().str.strip()  # Normalize column names\nprint(df.columns)  # Debugging step\n\n# Visualize missing data\nmsno.heatmap(df)\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n\n# Drop columns with &gt;50% missing values\ndf.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n\n# Check if \"SALARY\" exists before filling missing values\nif \"SALARY\" in df.columns:\n    df[\"SALARY\"].fillna(df[\"SALARY\"].median(), inplace=True)\nelse:\n    print(\"⚠️ Warning: 'SALARY' column not found in dataframe!\")\n\n# Check if \"INDUSTRY\" exists before filling missing values\nif \"INDUSTRY\" in df.columns:\n    df[\"INDUSTRY\"].fillna(\"Unknown\", inplace=True)\nelse:\n    print(\"⚠️ Warning: 'INDUSTRY' column not found in dataframe!\")\n\nprint(\"✅ Missing value handling complete.\")\n\n# delete duplicates\ndf = df.drop_duplicates(subset=[\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"])\nprint(\"Duplicates removed.\")\n\nAvailable columns in dataset: ['ID', 'LAST_UPDATED_DATE', 'LAST_UPDATED_TIMESTAMP', 'DUPLICATES', 'POSTED', 'EXPIRED', 'DURATION', 'SOURCE_TYPES', 'SOURCES', 'URL', 'ACTIVE_URLS', 'ACTIVE_SOURCES_INFO', 'TITLE_RAW', 'BODY', 'MODELED_EXPIRED', 'MODELED_DURATION', 'COMPANY', 'COMPANY_NAME', 'COMPANY_RAW', 'COMPANY_IS_STAFFING', 'EDUCATION_LEVELS', 'EDUCATION_LEVELS_NAME', 'MIN_EDULEVELS', 'MIN_EDULEVELS_NAME', 'MAX_EDULEVELS', 'MAX_EDULEVELS_NAME', 'EMPLOYMENT_TYPE', 'EMPLOYMENT_TYPE_NAME', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'IS_INTERNSHIP', 'SALARY', 'REMOTE_TYPE', 'REMOTE_TYPE_NAME', 'ORIGINAL_PAY_PERIOD', 'SALARY_TO', 'SALARY_FROM', 'LOCATION', 'CITY', 'CITY_NAME', 'COUNTY', 'COUNTY_NAME', 'MSA', 'MSA_NAME', 'STATE', 'STATE_NAME', 'COUNTY_OUTGOING', 'COUNTY_NAME_OUTGOING', 'COUNTY_INCOMING', 'COUNTY_NAME_INCOMING', 'MSA_OUTGOING', 'MSA_NAME_OUTGOING', 'MSA_INCOMING', 'MSA_NAME_INCOMING', 'NAICS2', 'NAICS2_NAME', 'NAICS3', 'NAICS3_NAME', 'NAICS4', 'NAICS4_NAME', 'NAICS5', 'NAICS5_NAME', 'NAICS6', 'NAICS6_NAME', 'TITLE', 'TITLE_NAME', 'TITLE_CLEAN', 'SKILLS', 'SKILLS_NAME', 'SPECIALIZED_SKILLS', 'SPECIALIZED_SKILLS_NAME', 'CERTIFICATIONS', 'CERTIFICATIONS_NAME', 'COMMON_SKILLS', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS', 'SOFTWARE_SKILLS_NAME', 'ONET', 'ONET_NAME', 'ONET_2019', 'ONET_2019_NAME', 'CIP6', 'CIP6_NAME', 'CIP4', 'CIP4_NAME', 'CIP2', 'CIP2_NAME', 'SOC_2021_2', 'SOC_2021_2_NAME', 'SOC_2021_3', 'SOC_2021_3_NAME', 'SOC_2021_4', 'SOC_2021_4_NAME', 'SOC_2021_5', 'SOC_2021_5_NAME', 'LOT_CAREER_AREA', 'LOT_CAREER_AREA_NAME', 'LOT_OCCUPATION', 'LOT_OCCUPATION_NAME', 'LOT_SPECIALIZED_OCCUPATION', 'LOT_SPECIALIZED_OCCUPATION_NAME', 'LOT_OCCUPATION_GROUP', 'LOT_OCCUPATION_GROUP_NAME', 'LOT_V6_SPECIALIZED_OCCUPATION', 'LOT_V6_SPECIALIZED_OCCUPATION_NAME', 'LOT_V6_OCCUPATION', 'LOT_V6_OCCUPATION_NAME', 'LOT_V6_OCCUPATION_GROUP', 'LOT_V6_OCCUPATION_GROUP_NAME', 'LOT_V6_CAREER_AREA', 'LOT_V6_CAREER_AREA_NAME', 'SOC_2', 'SOC_2_NAME', 'SOC_3', 'SOC_3_NAME', 'SOC_4', 'SOC_4_NAME', 'SOC_5', 'SOC_5_NAME', 'LIGHTCAST_SECTORS', 'LIGHTCAST_SECTORS_NAME', 'NAICS_2022_2', 'NAICS_2022_2_NAME', 'NAICS_2022_3', 'NAICS_2022_3_NAME', 'NAICS_2022_4', 'NAICS_2022_4_NAME', 'NAICS_2022_5', 'NAICS_2022_5_NAME', 'NAICS_2022_6', 'NAICS_2022_6_NAME']\nDropped unnecessary columns.\nIndex(['LAST_UPDATED_DATE', 'POSTED', 'EXPIRED', 'DURATION', 'SOURCE_TYPES',\n       'SOURCES', 'ACTIVE_SOURCES_INFO', 'TITLE_RAW', 'BODY',\n       'MODELED_EXPIRED',\n       ...\n       'NAICS_2022_2', 'NAICS_2022_2_NAME', 'NAICS_2022_3',\n       'NAICS_2022_3_NAME', 'NAICS_2022_4', 'NAICS_2022_4_NAME',\n       'NAICS_2022_5', 'NAICS_2022_5_NAME', 'NAICS_2022_6',\n       'NAICS_2022_6_NAME'],\n      dtype='object', length=118)\nMissing values before cleaning:\nLAST_UPDATED_DATE        0\nPOSTED                   0\nEXPIRED               7822\nDURATION             27294\nSOURCE_TYPES             0\n                     ...  \nNAICS_2022_4_NAME        0\nNAICS_2022_5             0\nNAICS_2022_5_NAME        0\nNAICS_2022_6             0\nNAICS_2022_6_NAME        0\nLength: 118, dtype: int64\nIndex(['LAST_UPDATED_DATE', 'POSTED', 'EXPIRED', 'DURATION', 'SOURCE_TYPES',\n       'SOURCES', 'ACTIVE_SOURCES_INFO', 'TITLE_RAW', 'BODY',\n       'MODELED_EXPIRED',\n       ...\n       'NAICS_2022_2', 'NAICS_2022_2_NAME', 'NAICS_2022_3',\n       'NAICS_2022_3_NAME', 'NAICS_2022_4', 'NAICS_2022_4_NAME',\n       'NAICS_2022_5', 'NAICS_2022_5_NAME', 'NAICS_2022_6',\n       'NAICS_2022_6_NAME'],\n      dtype='object', length=118)\n\n\n\n\n\n\n\n\n\n⚠️ Warning: 'SALARY' column not found in dataframe!\n⚠️ Warning: 'INDUSTRY' column not found in dataframe!\n✅ Missing value handling complete.\nDuplicates removed.\n\n\n\nif \"SALARY\" in df.columns:\n    df[\"SALARY\"].fillna(df[\"SALARY\"].median(), inplace=True)\nelse:\n    print(\"⚠️ Warning: 'SALARY' column not found in dataframe!\")\n\n⚠️ Warning: 'SALARY' column not found in dataframe!\n\n\n\n# identifying data analyst jobs by keyword searching\nkeywords = ['Data Analyst', 'Business Analyst', 'Data Engineering', 'Deep Learning',\n            'Data Science', 'Data Analysis','Data Analytics',  'Market Research Analyst' \n            'LLM', 'Language Model', 'NLP', 'Natural Language Processing',\n            'Computer Vision', 'Business Intelligence Analyst', 'Quantitative Analyst', 'Operations Analyst']\n\nmatch = lambda col: df[col].str.contains('|'.join(keywords), case=False, na=False)\n\ndf['DATA_ANALYST_JOB'] = match('TITLE_NAME') \\\n             | match('SKILLS_NAME') \\\n             | match('SPECIALIZED_SKILLS_NAME') \ndf['DATA_ANALYST_JOB'].value_counts()\n\nDATA_ANALYST_JOB\nFalse    37052\nTrue     32148\nName: count, dtype: int64\n\n\n\ndf['DATA_ANALYST_JOB']\n\n0        False\n1        False\n2         True\n3        False\n4        False\n         ...  \n72471     True\n72472     True\n72473     True\n72474     True\n72475    False\nName: DATA_ANALYST_JOB, Length: 69200, dtype: bool\n\n\n\nimport plotly.express as px\n\n# Define custom colors for 'True' and 'False' categories\ncolor_map = {\n    False: \"#d62728\" ,   # Red\n    True: \"#3BB143 \"   # Green\n}\n\n\nfig = px.bar(df_grouped, \n             x='NAICS2_NAME', \n             y='Job_Count',\n             color='DATA_ANALYST_JOB',\n             title=\"Data Analytics & Business Analytics Job Trends\",\n              labels={'NAICS2_NAME': 'Industry', 'Job_Count': 'Number of Jobs'},\n             barmode='group',\n             color_discrete_map=color_map  # Custom colors\n            )\n\nfig.update_layout(\n    yaxis=dict(\n        title=\"Number of Jobs\", \n        range=[0, df_grouped['Job_Count'].max() * 1.2]  # Increase y-axis height\n    ),\n    height=700,  # Make the figure taller\n    xaxis=dict(\n        tickangle=-45  # Rotate x-axis labels for better readability\n    )\n)\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport plotly.express as px\n\n# Identify the top 2 industries with the most data analyst jobs\ntop_industries = df_grouped.groupby(\"NAICS2_NAME\")[\"Job_Count\"].sum().nlargest(10).index\n\n# Filter the original grouped DataFrame, not the raw df\ndf_top_industries = df_grouped[df_grouped[\"NAICS2_NAME\"].isin(top_industries)]\n\n# Create the bar chart to analyze job counts within these industries\nfig = px.bar(df_top_industries, \n             x=\"DATA_ANALYST_JOB\", \n             y=\"Job_Count\", \n             color=\"NAICS2_NAME\",\n             title=\"Top 2 Industries Hiring Data Analysts\",\n             labels={'DATA_ANALYST_JOB': 'Job Title', 'Job_Count': 'Number of Jobs'},\n             barmode='group')\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n# Define target industries\ntarget_industries = [\"Finance and Insurance\", \"Health Care and Social Assistance\"]\n\n# Filter the DataFrame\ndf_skills = df[df[\"NAICS2_NAME\"].isin(target_industries)]\n\n\nimport plotly.express as px\n\n# Define target industries\ntarget_industries = [\"Finance and Insurance\", \"Health Care and Social Assistance\"]\n\n# Filter the original DataFrame (df) instead of df_grouped\ndf_remote = df[df[\"NAICS2_NAME\"].isin(target_industries)]\n\n# Count occurrences of each remote type in these industries\ndf_remote_grouped = df_remote.groupby(\"REMOTE_TYPE_NAME\").size().reset_index(name=\"Count\")\n\n# Create a pie chart\nfig = px.pie(df_remote_grouped, \n             names=\"REMOTE_TYPE_NAME\", \n             values=\"Count\", \n             title=\"Remote vs. On-Site Data Analyst Jobs in Finance & Healthcare\",\n             color=\"REMOTE_TYPE_NAME\", \n             color_discrete_map={\"Remote\": \"#1f77b4\", \"On-Site\": \"#d62728\", \"Hybrid\": \"#2ca02c\"})\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Trends in Data Science & Business Analytics!",
    "section": "",
    "text": "The rise of AI is creating both fear and opportunity in the job market. According to the article, 86% of workers worry about job losses due to AI, with some predicting major shifts in the next two to five years. However, business leaders argue that AI isn’t just about replacing jobs—it’s about transforming them. For instance, Rakuten has partnered with OpenAI to create an internal version of ChatGPT, allowing employees to work more efficiently rather than losing their jobs. This highlights how AI is changing the workplace, not necessarily eliminating roles entirely (Samuels (2024)).\nA key trend in 2024 is the growing demand for AI-related skills across different industries. The article mentions that many organizations are now investing in AI, with companies expected to spend an average of $2.5 million on AI integration this year. As a result, job descriptions are evolving, requiring workers to adapt. For example, in software development, GitHub Copilot has made programmers 55% more efficient, allowing them to focus on complex problem-solving rather than repetitive coding. This shift means employees who upskill and learn to work alongside AI will have better career prospects.\nThe overall career outlook for business analytics and data science professionals remains positive, as companies look for ways to combine AI with human decision-making. The CEO of Nash Squared, Bev White, emphasizes that AI won’t just take jobs away but will reshape them, making workplaces more productive and allowing professionals to focus on meaningful, strategic tasks. Similarly, PepsiCo’s CIO, Nigel Richardson, believes that while some jobs will be replaced, AI will ultimately create more opportunities than it eliminates. The key takeaway is that success in this AI-driven era will depend on embracing change, reskilling, and staying adaptable rather than resisting technological advancements.\nWe will add more on this later\n\n\n\n\nReferences\n\nSamuels, M. (2024): “AI’s Employment Impact: 86,”https://www.zdnet.com/article/ai-employment-impact-86-of-workers-fear-job-losses-but-heres-some-good-news/."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Group 11: Trends in Data Science & Business Analytics",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\ndf = pd.read_csv(\"lightcast_job_postings.csv\")\nprint(\"Available columns in dataset:\", df.columns.tolist())\n\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\",\n    \"SOC_2\", \"SOC_3\", \"SOC_5\"\n]\n\ndf.drop(columns=columns_to_drop, inplace=True)\nprint(\"Dropped unnecessary columns.\")\nprint(df.columns)\n\n# handle missing value\nprint(\"Missing values before cleaning:\")\nprint(df.isnull().sum())\n\n\n\nimport missingno as msno\nimport matplotlib.pyplot as plt\n\n# Check column names\ndf.columns = df.columns.str.upper().str.strip()  # Normalize column names\nprint(df.columns)  # Debugging step\n\n# Visualize missing data\nplt.figure(figsize=(1, 0.5))\nmsno.heatmap(df)\nplt.title(\"Missing Values Heatmap\")\nplt.show()\n\n# Drop columns with &gt;50% missing values\ndf.dropna(thresh=len(df) * 0.5, axis=1, inplace=True)\n\n# Check if \"SALARY\" exists before filling missing values\nif \"SALARY\" in df.columns:\n    df[\"SALARY\"].fillna(df[\"SALARY\"].median(), inplace=True)\nelse:\n    print(\"⚠️ Warning: 'SALARY' column not found in dataframe!\")\n\n# Check if \"INDUSTRY\" exists before filling missing values\nif \"INDUSTRY\" in df.columns:\n    df[\"INDUSTRY\"].fillna(\"Unknown\", inplace=True)\nelse:\n    print(\"⚠️ Warning: 'INDUSTRY' column not found in dataframe!\")\n\nprint(\"✅ Missing value handling complete.\")\n\n# delete duplicates\ndf = df.drop_duplicates(subset=[\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"])\nprint(\"Duplicates removed.\")\n\n\nRunning cells with 'Python 3.13.1' requires the ipykernel package.\n\nRun the following command to install 'ipykernel' into the Python environment. \n\nCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'\n\n\n\n\nif \"SALARY\" in df.columns:\n    df[\"SALARY\"].fillna(df[\"SALARY\"].median(), inplace=True)\nelse:\n    print(\"⚠️ Warning: 'SALARY' column not found in dataframe!\")\n\n⚠️ Warning: 'SALARY' column not found in dataframe!\n\n\n\n# identifying data analyst jobs by keyword searching\nkeywords = ['Data Analyst', 'Business Analyst', 'Data Engineering', 'Deep Learning',\n            'Data Science', 'Data Analysis','Data Analytics',  'Market Research Analyst' \n            'LLM', 'Language Model', 'NLP', 'Natural Language Processing',\n            'Computer Vision', 'Business Intelligence Analyst', 'Quantitative Analyst', 'Operations Analyst']\n\nmatch = lambda col: df[col].str.contains('|'.join(keywords), case=False, na=False)\n\ndf['DATA_ANALYST_JOB'] = match('TITLE_NAME') \\\n             | match('SKILLS_NAME') \\\n             | match('SPECIALIZED_SKILLS_NAME') \ndf['DATA_ANALYST_JOB'].value_counts()\n\n\nDATA_ANALYST_JOB\nFalse    37052\nTrue     32148\nName: count, dtype: int64\n\n\n\ndf['DATA_ANALYST_JOB']\n\n0        False\n1        False\n2         True\n3        False\n4        False\n         ...  \n72471     True\n72472     True\n72473     True\n72474     True\n72475    False\nName: DATA_ANALYST_JOB, Length: 69200, dtype: bool\n\n\n\nimport plotly.express as px\n\n# Group data\ndf_grouped = df.groupby(['DATA_ANALYST_JOB', 'NAICS2_NAME']).size().reset_index(name='Job_Count')\n\n# Define custom colors for 'True' and 'False' categories\ncolor_map = {\n    False: \"#d62728\",   # Red\n    True: \"#3BB143\"     # Green (Fixed extra space issue)\n}\n\n# Create the figure\nfig = px.bar(df_grouped, \n             x='NAICS2_NAME', \n             y='Job_Count',\n             color='DATA_ANALYST_JOB',\n             title=\"Data Analytics & Business Analytics Job Trends\",\n             labels={'NAICS2_NAME': 'Industry', 'Job_Count': 'Number of Jobs'},\n             barmode='group',\n             color_discrete_map=color_map  # Custom colors\n            )\n\n# **Fix sizing issues**\nfig.update_layout(\n    autosize=True,  # Let it adapt dynamically\n    width=1200,     # Set wider width\n    height=700,     # Set taller height\n    margin=dict(l=60, r=60, t=50, b=100),  # Avoid excessive padding\n    font=dict(size=14),  # Increase text size\n    xaxis=dict(\n        tickangle=-45,  # Rotate x-axis labels\n        title_font=dict(size=16),  # Bigger title\n        tickfont=dict(size=14)  # Bigger tick labels\n    ),\n    yaxis=dict(\n        title=\"Number of Jobs\", \n        range=[0, df_grouped['Job_Count'].max() * 1.2],  # Extend Y-axis range\n        title_font=dict(size=16),  # Bigger title\n        tickfont=dict(size=14)  # Bigger tick labels\n    )\n)\n\n# Show the plot\nfig.show()\n\n# Save as an HTML file for Quarto embedding\nfig.write_html(\"figures/plot1_jobtrends.html\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport plotly.express as px\n\n# Identify the top 10 industries with the most data analyst jobs\ntop_industries = df_grouped.groupby(\"NAICS2_NAME\")[\"Job_Count\"].sum().nlargest(10).index\n\n# Filter the dataset for top industries\ndf_top_industries = df_grouped[df_grouped[\"NAICS2_NAME\"].isin(top_industries)]\n\n# Create the bar chart\nfig = px.bar(df_top_industries, \n             x=\"DATA_ANALYST_JOB\", \n             y=\"Job_Count\", \n             color=\"NAICS2_NAME\",\n             title=\"Top 10 Industries Hiring Data Analysts\",\n             labels={'DATA_ANALYST_JOB': 'Job Title', 'Job_Count': 'Number of Jobs'},\n             barmode='group')\n\n# **Fix sizing & layout**\nfig.update_layout(\n    autosize=True,\n    width=1100,  # Adjust width to fit layout\n    height=600,  # Increase height\n    margin=dict(l=50, r=50, t=50, b=50),  # Reduce margins\n    font=dict(size=14),  # Improve font readability\n    xaxis=dict(tickangle=-45),  # Rotate x-axis labels for better fit\n)\nfig.show()\n# Save the updated plot as an HTML file\nfig.write_html(\"figures/plot2industries.html\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n# Define target industries\ntarget_industries = [\"Finance and Insurance\", \"Health Care and Social Assistance\"]\n\n# Filter the DataFrame\ndf_skills = df[df[\"NAICS2_NAME\"].isin(target_industries)]\n\n\nimport plotly.express as px\n\n# Define target industries\ntarget_industries = [\"Finance and Insurance\", \"Health Care and Social Assistance\"]\n\n# Filter the original DataFrame (df) instead of df_grouped\ndf_remote = df[df[\"NAICS2_NAME\"].isin(target_industries)]\n\n# Count occurrences of each remote type in these industries\ndf_remote_grouped = df_remote.groupby(\"REMOTE_TYPE_NAME\").size().reset_index(name=\"Count\")\n\n# Create a pie chart\nfig = px.pie(df_remote_grouped, \n             names=\"REMOTE_TYPE_NAME\", \n             values=\"Count\", \n             title=\"Remote vs. On-Site Data Analyst Jobs in Finance & Healthcare\",\n             color=\"REMOTE_TYPE_NAME\", \n             color_discrete_map={\"Remote\": \"#1f77b4\", \"On-Site\": \"#d62728\", \"Hybrid\": \"#2ca02c\"})\n\nfig.show()\nfig.write_html(\"plot3pie.html\")\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  }
]